{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 1:** Importanción de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suprimiendo los Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando las librerías para trabajar CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D        \n",
    "from keras.layers import MaxPooling2D       \n",
    "from keras.layers import Flatten            \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2:** Construcción de la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea y se inicializa la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "#Paso 1 - Convolución------------------------------------\n",
    "classifier.add(Convolution2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3,3),\n",
    "    input_shape = (64,64,3),\n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "#Paso 2 - Max Pooling------------------------------------\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Se agrega una segunda capa de convolución y de max pooling para mejorar la red\n",
    "classifier.add(Convolution2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3,3),\n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Se agrega una tercer capa de convolución y de max pooling para mejorar la red\n",
    "classifier.add(Convolution2D(\n",
    "    filters=128,\n",
    "    kernel_size=(3,3),\n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Paso 3 - Flattening-------------------------------------\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#Paso 4 - Full Connection--------------------------------\n",
    "classifier.add(Dense(\n",
    "    units=64,\n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "classifier.add(Dropout(rate=0.5))\n",
    "\n",
    "classifier.add(Dense(#Capa de salida\n",
    "    units=1,\n",
    "    activation=\"sigmoid\"\n",
    "))\n",
    "\n",
    "#Compilación de la CNN\n",
    "classifier.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3:** Ajustar la imágenes para entrenar la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 177s 879ms/step - loss: 0.6935 - accuracy: 0.5063 - val_loss: 0.6923 - val_accuracy: 0.5010\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 98s 488ms/step - loss: 0.6846 - accuracy: 0.5491 - val_loss: 0.6750 - val_accuracy: 0.5995\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 99s 494ms/step - loss: 0.6645 - accuracy: 0.5940 - val_loss: 0.6398 - val_accuracy: 0.6325\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 103s 513ms/step - loss: 0.6223 - accuracy: 0.6594 - val_loss: 0.5933 - val_accuracy: 0.6860\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 93s 463ms/step - loss: 0.5849 - accuracy: 0.7024 - val_loss: 0.5396 - val_accuracy: 0.7300\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.5434 - accuracy: 0.7372 - val_loss: 0.5123 - val_accuracy: 0.7465\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 91s 452ms/step - loss: 0.5112 - accuracy: 0.7566 - val_loss: 0.4750 - val_accuracy: 0.7760\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 90s 447ms/step - loss: 0.4912 - accuracy: 0.7709 - val_loss: 0.4707 - val_accuracy: 0.7735\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 91s 455ms/step - loss: 0.4810 - accuracy: 0.7776 - val_loss: 0.4968 - val_accuracy: 0.7680\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 90s 448ms/step - loss: 0.4514 - accuracy: 0.7926 - val_loss: 0.4192 - val_accuracy: 0.8100\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 89s 446ms/step - loss: 0.4351 - accuracy: 0.8059 - val_loss: 0.4219 - val_accuracy: 0.8020\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 89s 446ms/step - loss: 0.4275 - accuracy: 0.8111 - val_loss: 0.4198 - val_accuracy: 0.8045\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 91s 453ms/step - loss: 0.4166 - accuracy: 0.8155 - val_loss: 0.3832 - val_accuracy: 0.8295\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 90s 447ms/step - loss: 0.4029 - accuracy: 0.8198 - val_loss: 0.3925 - val_accuracy: 0.8300\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 89s 445ms/step - loss: 0.3855 - accuracy: 0.8295 - val_loss: 0.4265 - val_accuracy: 0.8045\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 118s 591ms/step - loss: 0.3769 - accuracy: 0.8347 - val_loss: 0.4092 - val_accuracy: 0.8195\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 139s 691ms/step - loss: 0.3668 - accuracy: 0.8389 - val_loss: 0.3780 - val_accuracy: 0.8310\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 135s 671ms/step - loss: 0.3803 - accuracy: 0.8305 - val_loss: 0.3883 - val_accuracy: 0.8315\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 112s 556ms/step - loss: 0.3454 - accuracy: 0.8462 - val_loss: 0.4234 - val_accuracy: 0.8055\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 109s 547ms/step - loss: 0.3494 - accuracy: 0.8480 - val_loss: 0.4155 - val_accuracy: 0.8240\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 114s 571ms/step - loss: 0.3373 - accuracy: 0.8514 - val_loss: 0.4110 - val_accuracy: 0.8310\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 92s 459ms/step - loss: 0.3211 - accuracy: 0.8650 - val_loss: 0.3751 - val_accuracy: 0.8450\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 93s 463ms/step - loss: 0.3195 - accuracy: 0.8599 - val_loss: 0.3900 - val_accuracy: 0.8405\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 0.3046 - accuracy: 0.8670 - val_loss: 0.3688 - val_accuracy: 0.8325\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 91s 452ms/step - loss: 0.3086 - accuracy: 0.8708 - val_loss: 0.3696 - val_accuracy: 0.8480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26fdb8a0550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory(\n",
    "    directory='dataset/training_set/',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=40,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory(\n",
    "    directory='dataset/test_set/',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=40,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "classifier.fit_generator(\n",
    "    training_dataset,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=25,\n",
    "    validation_data=testing_dataset,\n",
    "    validation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 4:** Realizar una simple predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 576ms/step\n"
     ]
    }
   ],
   "source": [
    "#Carga la imagen de prueba\n",
    "imagen_prueba = image.load_img(\n",
    "    'dataset/single_prediction/cat_or_dog_1.jpg',\n",
    "    target_size = (64, 64)\n",
    ")\n",
    "\n",
    "#Convierte la imagen a un array numpy\n",
    "imagen_array = image.img_to_array(imagen_prueba)\n",
    "imagen_array = np.expand_dims(imagen_array, axis=0)\n",
    "\n",
    "# Normalizar la imagen\n",
    "imagen_array /= 255.0\n",
    "\n",
    "#Realizar la predicción\n",
    "prediccion = classifier.predict(imagen_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procede a verificar cómo están etiquetadas las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Si la probabilidad predicha es cercana a 1, el modelo está prediciendo con alta confianza la clase etiquetada como 1 ('dogs').\n",
    "* Si la probabilidad predicha es cercana a 0, el modelo está prediciendo con alta confianza la clase etiquetada como 0 ('cats')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusión:** Dado que el resultado de la predicción es 0.99999756 y la clase 'dogs' ('perros') \n",
    "está etiquetada como 1, puedes interpretar esa predicción como altamente indicativa \n",
    "de que la imagen pertenece a la clase 'dogs' ('perros'). En este caso, una probabilidad \n",
    "cercana a 1 sugiere una alta confianza del modelo en que la imagen es un perro según la \n",
    "configuración de etiquetas y clases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
