{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 1:** Importanción de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suprimiendo los Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando las librerías para trabajar CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D        \n",
    "from keras.layers import MaxPooling2D       \n",
    "from keras.layers import Flatten            \n",
    "from keras.layers import Dense \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2:** Construcción de la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea y se inicializa la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "#Paso 1 - Convolución\n",
    "classifier.add(Convolution2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3,3),\n",
    "    input_shape = (64,64,3),\n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "#Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(\n",
    "    pool_size=(2,2)\n",
    "))\n",
    "\n",
    "#Se agrega otra capa de convolución y de max pooling para mejorar la red\n",
    "classifier.add(Convolution2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3,3),\n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "classifier.add(MaxPooling2D(\n",
    "    pool_size=(2,2)\n",
    "))\n",
    "\n",
    "#Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#Paso 4 - Full Connection\n",
    "classifier.add(Dense(\n",
    "    units=128,\n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "classifier.add(Dense(\n",
    "    units=1,\n",
    "    activation=\"sigmoid\"\n",
    "))\n",
    "\n",
    "#Compilación de la CNN\n",
    "classifier.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3:** Ajustar la imágenes para entrenar la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 111s 549ms/step - loss: 0.6847 - accuracy: 0.5541 - val_loss: 0.6297 - val_accuracy: 0.6640\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 88s 440ms/step - loss: 0.6208 - accuracy: 0.6555 - val_loss: 0.6324 - val_accuracy: 0.6430\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 102s 508ms/step - loss: 0.5816 - accuracy: 0.6986 - val_loss: 0.6299 - val_accuracy: 0.6625\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 119s 589ms/step - loss: 0.5384 - accuracy: 0.7266 - val_loss: 0.5724 - val_accuracy: 0.7065\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 87s 435ms/step - loss: 0.5241 - accuracy: 0.7399 - val_loss: 0.4962 - val_accuracy: 0.7675\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 80s 399ms/step - loss: 0.4901 - accuracy: 0.7638 - val_loss: 0.4882 - val_accuracy: 0.7790\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.4716 - accuracy: 0.7703 - val_loss: 0.4762 - val_accuracy: 0.7750\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 79s 392ms/step - loss: 0.4584 - accuracy: 0.7819 - val_loss: 0.5129 - val_accuracy: 0.7535\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 75s 375ms/step - loss: 0.4432 - accuracy: 0.7906 - val_loss: 0.4903 - val_accuracy: 0.7675\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 73s 366ms/step - loss: 0.4280 - accuracy: 0.7970 - val_loss: 0.4566 - val_accuracy: 0.7915\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 72s 357ms/step - loss: 0.4191 - accuracy: 0.8019 - val_loss: 0.4511 - val_accuracy: 0.7975\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 72s 358ms/step - loss: 0.4077 - accuracy: 0.8129 - val_loss: 0.4600 - val_accuracy: 0.7865\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 75s 375ms/step - loss: 0.3980 - accuracy: 0.8148 - val_loss: 0.4431 - val_accuracy: 0.7970\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 73s 365ms/step - loss: 0.3852 - accuracy: 0.8221 - val_loss: 0.4755 - val_accuracy: 0.7975\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 72s 359ms/step - loss: 0.3825 - accuracy: 0.8241 - val_loss: 0.4479 - val_accuracy: 0.7930\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 72s 358ms/step - loss: 0.3751 - accuracy: 0.8299 - val_loss: 0.4403 - val_accuracy: 0.8065\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 72s 357ms/step - loss: 0.3654 - accuracy: 0.8345 - val_loss: 0.4455 - val_accuracy: 0.8055\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 73s 362ms/step - loss: 0.3469 - accuracy: 0.8495 - val_loss: 0.4674 - val_accuracy: 0.7880\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 0.3431 - accuracy: 0.8485 - val_loss: 0.4511 - val_accuracy: 0.8070\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 0.3295 - accuracy: 0.8549 - val_loss: 0.5015 - val_accuracy: 0.7870\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 73s 362ms/step - loss: 0.3273 - accuracy: 0.8533 - val_loss: 0.4692 - val_accuracy: 0.8020\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 72s 361ms/step - loss: 0.3190 - accuracy: 0.8583 - val_loss: 0.5059 - val_accuracy: 0.7860\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 72s 358ms/step - loss: 0.3106 - accuracy: 0.8666 - val_loss: 0.4994 - val_accuracy: 0.8050\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 72s 360ms/step - loss: 0.3050 - accuracy: 0.8716 - val_loss: 0.4439 - val_accuracy: 0.8145\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 71s 357ms/step - loss: 0.2943 - accuracy: 0.8726 - val_loss: 0.5000 - val_accuracy: 0.7980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f67b5b9a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory(\n",
    "    directory='dataset/training_set/',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=40,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory(\n",
    "    directory='dataset/test_set/',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=40,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "classifier.fit_generator(\n",
    "    training_dataset,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=25,\n",
    "    validation_data=testing_dataset,\n",
    "    validation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 4:** Realizar una simple predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    }
   ],
   "source": [
    "#Carga la imagen de prueba\n",
    "imagen_prueba = image.load_img(\n",
    "    'dataset/single_prediction/cat_or_dog_1.jpg',\n",
    "    target_size = (64, 64)\n",
    ")\n",
    "\n",
    "#Convierte la imagen a un array numpy\n",
    "imagen_array = image.img_to_array(imagen_prueba)\n",
    "imagen_array = np.expand_dims(imagen_array, axis=0)\n",
    "\n",
    "# Normalizar la imagen\n",
    "imagen_array /= 255.0\n",
    "\n",
    "#Realizar la predicción\n",
    "prediccion = classifier.predict(imagen_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procede a verificar cómo están etiquetadas las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Si la probabilidad predicha es cercana a 1, el modelo está prediciendo con alta confianza la clase etiquetada como 1 ('dogs').\n",
    "* Si la probabilidad predicha es cercana a 0, el modelo está prediciendo con alta confianza la clase etiquetada como 0 ('cats')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999756"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusión:** Dado que el resultado de la predicción es 0.99999756 y la clase 'dogs' ('perros') \n",
    "está etiquetada como 1, puedes interpretar esa predicción como altamente indicativa \n",
    "de que la imagen pertenece a la clase 'dogs' ('perros'). En este caso, una probabilidad \n",
    "cercana a 1 sugiere una alta confianza del modelo en que la imagen es un perro según la \n",
    "configuración de etiquetas y clases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
